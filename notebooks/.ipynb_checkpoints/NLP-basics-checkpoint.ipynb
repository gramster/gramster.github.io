{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Basics\n",
    "\n",
    "date: 2019-04-29T16:40:00\n",
    "author: Graham Wheeler\n",
    "category: Data Science\n",
    "comments: enabled\n",
    "draft: true\n",
    "tags: Python, Jupyter, Pandas, Data Science, Machine Learning\n",
    "\n",
    "<!--eofm-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This is the fifth post in a series based off my [Python for Data Science bootcamp](https://github.com/gramster/pythonbootcamp). The other posts are:*\n",
    "\n",
    "- *[a Python crash course](/post/python-crash-course/)*\n",
    "- *[using Jupyter](/post/using-jupyter/)*\n",
    "- *[exploratory data analysis](/post/exploratory-data-analysis-with-numpy-and-pandas/).*\n",
    "- *[introductory machine learning](/post/basic-machine-learning/).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post we will take a look at NLP - natural language processing - namely how we can apply ML techniques to collections of text (which we call _corpuses_ or maybe that should be _corpii_?).\n",
    "\n",
    "We've touched on this topic before but will go in more detail here.\n",
    "\n",
    "There are a number of applications for NLP, including:\n",
    "\n",
    "- [sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis) - is the text saying positive or negative things? There are many reasons we may want to know this. A common use case is monitoring social media like Twitter and seeing if people are expressing positive or negative opinions about a company and how the trend is changing over time. If you're representing the company on social media, or you're trading the stock of the company, this is very useful info.\n",
    "- [named-entity recognition (NER)](https://en.wikipedia.org/wiki/Named-entity_recognition) - what people, places and things are mentioned in the text?\n",
    "- [topic modeling](https://en.wikipedia.org/wiki/Topic_model) or [text summarization](https://en.wikipedia.org/wiki/Automatic_summarization) - what is the text saying? Topic modeling is just broad classification - for example, \"Is this text about sports?\", while text summarization is trying to extract the most salient points from the text.\n",
    "- [text generation](https://en.wikipedia.org/wiki/Natural-language_generation) - we can train models to generate text in different styles or on various topics\n",
    "- auto-responder bots - combining some of the above techniques, we can build bots to do, for example, first-line product support\n",
    "\n",
    "Before applying an NLP algorithm, we need to prepare the textual data. This includes a number of steps:\n",
    "\n",
    "- _cleaning_ the data. If we are using word-level representation, we are going to want to map each word into a numeric representation (e.g. a vector), and to do this we would want to restrict ourselves in most cases to a finite set of allowable words (the _vocabulary_). In order to reduce the size of the vocabulary we typically will do some cleaning/preprocessing. This can include removing punctuation and \"filler\" words like \"the\" that aren't needed to understand the text; we call these _stop words_. We may also want to standardize the form of words to reduce the number of variations which we can do with _stemming_ and _lemmatization_.\n",
    "- to represent text in a way amenable to ML algorithms, we need to encode it in some form of numeric vector. Depending on the algorithm, we may care about the order or the words, or in simple cases, perhaps we can simply use a set. In between these extremes we could have an unordered list of the words but with each word associated with some score for how important it is, or we could focus only on order for short sequences like adjacent word pairs (2-grams) or triplets (3-grams). If using a score, this could be as simple as the word count, or it could be a more sophisticated measure like the _TF_IDF_ score.\n",
    "\n",
    "We'll discuss each of these further in the next sections.\n",
    "\n",
    "## Python Libraries for NLP\n",
    "\n",
    "The most well-known Python NLP library is the _Natural Language Toolkit_ or _NLTK_ (https://www.nltk.org/). This has been around for number of years and has well-written code that is great for teaching the concepts. It is not the most performant though, and focuses on classical approaches. More recently, spaCy (https://spacy.io/) has become popular, as it is a more modern library that is optimized for use in actual production environments.\n",
    "\n",
    "Very recently there has been a plethora of new libraries, mostly based on PyTorch, such as:\n",
    "\n",
    "- PyText (https://github.com/facebookresearch/PyText) from Facebook\n",
    "- AllenNLP (https://allennlp.org/) from the Allen Institute for AI\n",
    "- Flair (https://github.com/zalandoresearch/flair) from Zalando Research\n",
    "\n",
    "For the purposes of this notebook we will stick to NLTK; once you understand the concepts it is not that difficult to transition to the other libraries.\n",
    "\n",
    "A few other useful libraries are worth calling out; these aren't for NLP per-se but could be very handy if you are doing NLP:\n",
    "\n",
    "- https://pypi.org/project/newspaper3k/ lets you extract articles from web sites into a structured form\n",
    "- Wikipedia's Python library makes wikipedia access a breeze: https://wikipedia.readthedocs.io/en/latest/quickstart.html\n",
    "- if you're dealing with parsing HTML data look at https://www.crummy.com/software/BeautifulSoup/\n",
    "\n",
    "\n",
    "## Preparing the Text\n",
    "\n",
    "### Removing Punctuation\n",
    "\n",
    "Note that punctuation can be significant - think of the difference between ending a sentence with ! vs ? - but that most of the techniques used in NLP will ignore it. Character-level convolutional neural networks are one way of making use of punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ill be there', 'Noooo']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import unicodedata\n",
    "\n",
    "punc = dict.fromkeys(i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "data = [\n",
    "    \"I'll be there!\",\n",
    "    \"N-o-o-o-o!!\"\n",
    "]\n",
    "    \n",
    "new_data = [s.translate(punc) for s in data]\n",
    "\n",
    "new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Letter Case\n",
    "\n",
    "It's very common to normalize letter case when dealing with text (i.e. making everything upper case or lower case rather than mixed case). Whether you should do this and which case to use may depend on the context and the libraries you are using. For NLTK you generally want to use lower-case for everything.\n",
    "\n",
    "A motivation for normalizing case is that the data we get is often dirty, with incorrect or mixed-capitalization, so normalizing at least makes it consistent. But we do lose information in the process, particularly to distinguish proper from regular nouns. \n",
    "\n",
    "### Tokenizing Words and Splitting Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/grwheele/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install the prerequisites\n",
    "\n",
    "import nltk\n",
    "\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'cat', 'in', 'the', 'hat', '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(\"The cat in the hat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I will not eat them with a fox!', 'I will not eat them in a box.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "data = \"I will not eat them with a fox! I will not eat them in a box.\"\n",
    "\n",
    "sent_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I', 'will', 'not', 'eat', 'them', 'with', 'a', 'fox', '!'],\n",
       " ['I', 'will', 'not', 'eat', 'them', 'in', 'a', 'box', '.']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word_tokenize(s) for s in sent_tokenize(data)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if we want to do sentence tokenization we should _not_ remove punctuation beforehand. Instead we could do something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I', 'will', 'not', 'eat', 'them', 'with', 'a', 'fox'],\n",
       " ['I', 'will', 'not', 'eat', 'them', 'in', 'a', 'box']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word_tokenize(s.translate(punc)) for s in sent_tokenize(data)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop-Word Removal and Restricting to a Fixed Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/grwheele/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install the prerequisites\n",
    "\n",
    "import nltk\n",
    "\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['eat', 'fox'], ['eat', 'box']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# NLTK assumes words have been lower-cased\n",
    "\n",
    "data = \"I will not eat them with a fox! I will not eat them in a box.\"\n",
    "sentences = sent_tokenize(data)\n",
    "\n",
    "[[w for w in word_tokenize(s.translate(punc).lower()) if w not in stop] for s in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization\n",
    "\n",
    "Many similar words have common \"stems\". For example, \"geology\", \"geological\", \"geologically\" all have the stem \"geolog\". [Stemming](https://en.wikipedia.org/wiki/Stemming) is the process of reducing words to their stem forms - this reduces our vocabulary size with little loss of meaning. There are different algorithms for doing this; a common one is the Porter algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['geolog', 'speak', 'the', 'geolog', 'of', 'the', 'area', 'is', 'geolog']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "[stemmer.stem(w) for w in word_tokenize(\"geologically speaking the geology of the area is geological\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Lemmatization](https://en.wikipedia.org/wiki/Lemmatisation) is reducing similar words to their dictionary form which is context-dependent (including part-of-speech). This is more complex than stemming, which is a function just of the word without the associated context).\n",
    "\n",
    "Lemmatization is more complex that stemming and an area of open-research. It's also more accurate. So which one to use is dependent on the task at hand. For information retrieval tasks, recall may take priority over precision, so stemming can actually be better.\n",
    "\n",
    "NLTK has a lemmatizer based on WordNet:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/grwheele/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install the prerequisites\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['geologically',\n",
       " 'speaking',\n",
       " 'the',\n",
       " 'geology',\n",
       " 'of',\n",
       " 'the',\n",
       " 'area',\n",
       " 'is',\n",
       " 'geological']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "[lemmatizer.lemmatize(w) for w in word_tokenize(\"geologically speaking the geology of the area is geological\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from the above that while stemming can reduce the size of the corpus vocabularly a decent amount, lemmatization does not do so quite as much. Here's a bigger example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260 tokens\n",
      "152 unique tokens\n",
      "137 unique stems\n",
      "145 unique lemmas\n"
     ]
    }
   ],
   "source": [
    "# source: https://en.wikipedia.org/wiki/Natural-language_generation\n",
    "data = \"\"\"\n",
    "Natural-language generation (NLG) is the natural-language processing task of \n",
    "generating natural language from a machine-representation system such as a \n",
    "knowledge base or a logical form. Psycholinguists prefer the term language \n",
    "production when such formal representations are interpreted as models for\n",
    "mental representations.\n",
    "\n",
    "It could be said an NLG system is like a translator that converts data into\n",
    "a natural-language representation. However, the methods to produce the final\n",
    "language are different from those of a compiler due to the inherent expressivity\n",
    "of natural languages. NLG has existed for a long time but commercial NLG \n",
    "technology has only recently become widely available.\n",
    "\n",
    "NLG may be viewed as the opposite of natural-language understanding: whereas\n",
    "in natural-language understanding, the system needs to disambiguate the input\n",
    "sentence to produce the machine representation language, in NLG the system \n",
    "needs to make decisions about how to put a concept into words.\n",
    "\n",
    "A simple example is systems that generate form letters. These do not typically\n",
    "involve grammar rules, but may generate a letter to a consumer, e.g. stating\n",
    "that a credit card spending limit was reached. To put it another way, simple \n",
    "systems use a template not unlike a Word document mail merge, but more complex \n",
    "NLG systems dynamically create text. As in other areas of natural-language \n",
    "processing, this can be done using either explicit models of language (e.g.,\n",
    "grammars) and the domain, or using statistical models derived by analysing\n",
    "human-written texts.\"\"\"\n",
    "\n",
    "tokens = word_tokenize(data)\n",
    "print(f\"{len(tokens)} tokens\")\n",
    "print(f\"{len(set(tokens))} unique tokens\")\n",
    "print(f\"{len(set([stemmer.stem(w) for w in tokens]))} unique stems\")\n",
    "print(f\"{len(set([lemmatizer.lemmatize(w) for w in tokens]))} unique lemmas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling Parts of Speech\n",
    "\n",
    "In many cases we won't concern ourselves with parts of speech (POS); we will simply take our sequence of cleaned up words and turn that into a vector representation, which we will discuss in the next section. However, POS tagging can be helpful in disambiguating words that can have multiple meanings and grammatical purposes based on context. For example:\n",
    "\n",
    "- \"Look at the time\"\n",
    "- \"Time how long it took to look\"\n",
    "\n",
    "So for some models we can do POS labelling and then encode the POS tags as part of the model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/grwheele/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install the prerequisites\n",
    "import nltk\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('look', 'NN'),\n",
       " ('at', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('time', 'NN'),\n",
       " ('.', '.'),\n",
       " ('time', 'NN'),\n",
       " ('how', 'WRB'),\n",
       " ('long', 'JJ'),\n",
       " ('it', 'PRP'),\n",
       " ('took', 'VBD'),\n",
       " ('to', 'TO'),\n",
       " ('look', 'VB'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "pos_tag(word_tokenize(\"Look at the time. Time how long it took to look.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the different tags are described in the NLTK help which we can access with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /Users/grwheele/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping help/tagsets.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "nltk.download('tagsets')\n",
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at our test above you may notice the tagger didn't do that well. In the first sentence \"look\" is a verb but was tagged as a noun. Your mileage will vary with different libraries when applying many NLP algorithms; understanding natural language is a hard problem and has not been completely solved!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Representation\n",
    "\n",
    "Once we have our cleaned and canonicalized data, we need to turn it into a form suitable for consumption by an ML algorithm; i.e. as a vector of features. In this section we'll look at common ways of doing this.\n",
    "\n",
    "In most cases we need a vocabulary; that is, the complete set of words that we expect to see in our problem domain. We'll typically generate this from the training data, and then as part of our data prep for prediction, we will drop any new words that we did not see before and are not in our vocabulary. That means if we are working in a domain that changes frequently we should retrain often, to make our vocabularly reflect current usage.\n",
    "\n",
    "\n",
    "### Bag of Words\n",
    "\n",
    "One of the simplest representations is to use a frequency count vector to represent our input as a set of words across the vocabularly domain. We saw an example of this in the previous notebook. We repeat that here with slight refinement to remove stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ate</th>\n",
       "      <th>belonged</th>\n",
       "      <th>cat</th>\n",
       "      <th>hat</th>\n",
       "      <th>mat</th>\n",
       "      <th>rat</th>\n",
       "      <th>sat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ate  belonged  cat  hat  mat  rat  sat\n",
       "0    0         0    1    0    1    0    1\n",
       "1    0         1    0    0    1    1    0\n",
       "2    0         0    0    1    1    0    0\n",
       "3    1         0    1    0    0    1    0\n",
       "4    0         0    1    1    0    0    0\n",
       "5    0         0    2    1    1    1    0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the mat belonged to the rat\",\n",
    "    \"the hat was on the mat\",\n",
    "    \"the cat ate the rat\",\n",
    "    \"the cat now has the hat\",\n",
    "    \"cat hat, cat mat, no rat\"\n",
    "]\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "v = CountVectorizer(stop_words=\"english\")\n",
    "X = v.fit_transform(data)\n",
    "pd.DataFrame(X.toarray(), columns=v.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the total vocabulary from the column names:\n",
    "\n",
    "`ate belonged cat hat mat rat sat`\n",
    "\n",
    "and each row simply has a count in the corrresponding column of the number of times that word occurred in the sentence. Notice that by using this encoding approach we lose all order information.\n",
    "\n",
    "Be aware that as our corpus grows large, so does our vocabulary, and we can end up with very sparse vectors. There are ways of dealing with this but they introduce other limitations. Take a look at the `HashingVectorizer` as one possibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-Hot Encoding\n",
    "\n",
    "1-hot encoding (sometimes called 1-Hot or 1HE) is a similar represenation to Bag of Words except we use binary 0/1 instead of frequency counts. Another way of thinking of this is that Bag of Words is a multiset represenation of the sentence while 1-hot is a normal set. We just need to add a `binary=True` argument to the `CountVectorizer` constructor for this behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ate</th>\n",
       "      <th>belonged</th>\n",
       "      <th>cat</th>\n",
       "      <th>hat</th>\n",
       "      <th>mat</th>\n",
       "      <th>rat</th>\n",
       "      <th>sat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ate  belonged  cat  hat  mat  rat  sat\n",
       "0    0         0    1    0    1    0    1\n",
       "1    0         1    0    0    1    1    0\n",
       "2    0         0    0    1    1    0    0\n",
       "3    1         0    1    0    0    1    0\n",
       "4    0         0    1    1    0    0    0\n",
       "5    0         0    1    1    1    1    0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = CountVectorizer(stop_words=\"english\", binary=True)\n",
    "X = v.fit_transform(data)\n",
    "pd.DataFrame(X.toarray(), columns=v.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice a problem with the above examples; while we could specify stop words we didn't get to do stemming. In order to fix this we need to do some customization of the vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    _stemmer = nltk.stem.PorterStemmer()\n",
    "    \n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: [StemmedCountVectorizer._stemmer.stem(w) for w in analyzer(doc)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ate</th>\n",
       "      <th>belong</th>\n",
       "      <th>cat</th>\n",
       "      <th>hat</th>\n",
       "      <th>mat</th>\n",
       "      <th>rat</th>\n",
       "      <th>sat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ate  belong  cat  hat  mat  rat  sat\n",
       "0    0       0    1    0    1    0    1\n",
       "1    0       1    0    0    1    1    0\n",
       "2    0       0    0    1    1    0    0\n",
       "3    1       0    1    0    0    1    0\n",
       "4    0       0    1    1    0    0    0\n",
       "5    0       0    1    1    1    1    0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = StemmedCountVectorizer(stop_words=\"english\", binary=True)\n",
    "X = v.fit_transform(data)\n",
    "pd.DataFrame(X.toarray(), columns=v.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Significance Metrics\n",
    "\n",
    "The problem with word counts, especially if we don't remove stop words, is that common words get scored highly, which may not always be desirable. What we want to score highly are words that are common *in this instance* that are not common *across all instances*. That should weight words that are significant to the particular row/instance. We can do this with *[Term Frequency - Inverse Document Frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)* or TF-IDF scores. \n",
    "\n",
    "- _Term frequency_ is the frequency of each word (term) in the input; i.e. this is the same as the bag-of-words score we saw above, but scaled by the number of words in each sentence\n",
    "- _Document frequency_ is a measure of how common the term is across all instances; e.g. it could be the percentage of input instances that contain the term\n",
    "- _Inverse document frequency_ is a reciprocal measure of document frequency. It is typically the log of the reciprocal of the document frequency, with 1 added to the numerator and denominator to avoid divide-by-zero issues, and 1 added to the total to avoid zero-weighting IDFs.\n",
    "\n",
    "For example, let's say we have two sentences in our corpus D:\n",
    "\n",
    "```\n",
    "D = { \"cats like rats\", \"rats don't like cats\" }\n",
    "```\n",
    "\n",
    "Then our vocabulary is:\n",
    "\n",
    "```\n",
    "V = { \"cats\", \"don't\", \"like\", \"rats\" }\n",
    "```\n",
    "\n",
    "and our term frequencies are:\n",
    "\n",
    "```\n",
    "tf(\"cats\", \"cats like rats\") = 1 / 3\n",
    "tf(\"don't\", \"cats like rats\") = 0\n",
    "tf(\"like\", \"cats like rats\") = 1 / 3\n",
    "tf(\"rats\", \"cats like rats\") = 1 / 3\n",
    "tf(\"cats\", \"rats don't like rats\") = 1 / 4\n",
    "tf(\"don't\", \"rats don't like cats\") = 1 / 4\n",
    "tf(\"like\", \"rats don't like cats\") = 1 / 4\n",
    "tf(\"rats\", \"rats don't like cats\") = 1 / 4\n",
    "```\n",
    "\n",
    "while our document frequencies are:\n",
    "\n",
    "```\n",
    "df(\"cats\", D) = 2\n",
    "df(\"don't\", D) = 1\n",
    "df(\"like\", D) = 2\n",
    "df(\"rats\", D) = 2\n",
    "```\n",
    "\n",
    "and our inverse document frequencies are:\n",
    "\n",
    "```\n",
    "idf(\"cats\", D) = 1 + log(3 / (1 + df(\"cats\", D))) = 1 + log(3 / 3) = 1\n",
    "idf(\"don't\", D) = 1 + log(3 / (1 + df(\"don't\", D))) = 1 + log(3 / 2) = 1.176\n",
    "idf(\"like\", D) = 1 + log(3 / (1 + df(\"like\", D))) = 1 + log(3 / 3) = 1\n",
    "idf(\"rats\", D) = 1 + log(3 / (1 + df(\"rats\", D))) = 1 + log(3 / 3) = 1\n",
    "```\n",
    "\n",
    "(Note: the numerator is 3 as we have two \"documents\" and we add 1). You can see here that the word \"don't\" gets a higher score than the other words which is expected as it is the only workd that doesn't occur in both sentences.\n",
    "\n",
    "Then our TF-IDF scores are:\n",
    "\n",
    "```\n",
    "tf-idf(\"cats\", \"cats like rats\", D) = tf(\"cats\", \"cats like rats\") * idf(\"cats\", D) = 1/3 * 1 = 1/3\n",
    "tf-idf(\"don't\", \"cats like rats\", D) = tf(\"don't\", \"cats like rats\") * idf(\"don't\", D) = 0 * 1.176 = 0\n",
    "tf-idf(\"like\", \"cats like rats\", D) = tf(\"like\", \"cats like rats\") * idf(\"like\", D) = 1/3 * 1 = 1/3\n",
    "tf-idf(\"rats\", \"cats like rats\", D) = tf(\"rats\", \"cats like rats\") * idf(\"rats\", D) = 1/3 * 1 = 1/3\n",
    "tf-idf(\"cats\", \"rats don't like rats\", D) = tf(\"cats\", \"rats don't like cats\") * idf(\"cats\", D) = 1/4 * 1 = 1/4\n",
    "tf-idf(\"don't\", \"rats don't like cats\", D) = tf(\"don't\", \"rats don't like cats\") * idf(\"don't\", D) = 1/4 * 1.176 = 0.294\n",
    "tf-idf(\"like\", \"rats don't like cats\", D) = tf(\"like\", \"rats don't like cats\") * idf(\"like\", D) = 1/4 * 1 = 1/4\n",
    "tf-idf(\"rats\", \"rats don't like cats\", D) = tf(\"rats\", \"rats don't like cats\") * idf(\"rats\", D) = 1/4 * 1 = 1/4\n",
    "```\n",
    "\n",
    "From this you can see:\n",
    "\n",
    "- the word \"don't\" has no significance in the first sentence and higher-than average significance in the second sentence\n",
    "- the other words have the same weight in their respective sentences, but overall higher weight in the first sentence simply because it is shorter\n",
    "\n",
    "It's worth noting that second point because intuitively the word \"like\" should have the same score in both sentences, so TF-IDF scores are useful for identifying significant words within inputs but should be used with care when comparing significance across inputs.\n",
    "\n",
    "To do this in sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cats</th>\n",
       "      <th>don</th>\n",
       "      <th>like</th>\n",
       "      <th>rats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.577350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.448321</td>\n",
       "      <td>0.630099</td>\n",
       "      <td>0.448321</td>\n",
       "      <td>0.448321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cats       don      like      rats\n",
       "0  0.577350  0.000000  0.577350  0.577350\n",
       "1  0.448321  0.630099  0.448321  0.448321"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "data2 = [\n",
    "    \"cats like rats\",\n",
    "    \"rats don't like cats\",\n",
    "]\n",
    "v = TfidfVectorizer()\n",
    "X = v.fit_transform(data2)\n",
    "pd.DataFrame(X.toarray(), columns=v.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we see the same relative pattern but the scores are different to what we computed. `sklearn` does some additional normalization of scores which you can read about [here](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting).\n",
    "\n",
    "TF-IDF is a very commonly used approach to computing word salience scores. A more recent approach is \"contextual salience\" which you read about in [this paper](https://arxiv.org/abs/1803.08493)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordered Representation with n-grams, Character or Word Vector Sequences\n",
    "\n",
    "Works well if we have a finite length input - e.g. Twitter tweets. For unbounded text we would need to feed this in to our algorithm in chunks, so would need an approach that has some form of \"short-term memory\" like an LSTM NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Models using Classical Techniques\n",
    "\n",
    "If we have a limit on the size of the input sentences, we can use traditional techniques to build classifiers. A great example is Twitter with its 240 character limit. The main problem is the sparsity of the data.\n",
    "\n",
    "\n",
    "In reality we are often dealing with larger pieces of text and may have very variable size. In this case we typically are going to want to use neural network techniques where we can feed in the input using some form of sliding window and can use short-term memory (e.g. LSTM) for remembering prior context.\n",
    "\n",
    "\n",
    "## Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
