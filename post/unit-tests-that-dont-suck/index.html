<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:site_name" content="Graham Wheeler's Random Forest"><meta property="og:type" content="article"><meta property="og:image" content="http://www.grahamwheeler.com/img/forest.jpg"><meta property="twitter:image" content="http://www.grahamwheeler.com/img/forest.jpg"><meta name=title content="Unit Tests that Don't Suck"><meta property="og:title" content="Unit Tests that Don't Suck"><meta property="twitter:title" content="Unit Tests that Don't Suck"><meta name=description content="Thoughts on technology, management and math by Graham Wheeler"><meta property="og:description" content="Thoughts on technology, management and math by Graham Wheeler"><meta property="twitter:description" content="Thoughts on technology, management and math by Graham Wheeler"><meta property="twitter:card" content="summary"><meta name=keyword content="Management, Psychology, Data Science, Mathematics, Software Engineering"><link rel="shortcut icon" href=/img/favicon.ico><title>Unit Tests that Don't Suck-Graham Wheeler's Random Forest</title><link rel=canonical href=/post/unit-tests-that-dont-suck/><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/hugo-theme-cleanwhite.min.css><link rel=stylesheet href=/css/zanshang.css><link href=https://cdn.jsdelivr.net/gh/FortAwesome/Font-Awesome@5.15.1/css/all.css rel=stylesheet type=text/css><script src=/js/jquery.min.js></script>
<script src=/js/bootstrap.min.js></script>
<script src=/js/hux-blog.min.js></script></head><nav class="navbar navbar-default navbar-custom navbar-fixed-top"><div class=container-fluid><div class="navbar-header page-scroll"><button type=button class=navbar-toggle>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span></button>
<a class=navbar-brand href=/>Graham Wheeler's Random Forest</a></div><div id=huxblog_navbar><div class=navbar-collapse><ul class="nav navbar-nav navbar-right"><li><a href=/>All Posts</a></li><li><a href=/top/books/>BOOKS</a></li><li><a href=/top/archive/>ARCHIVE</a></li><li><a href=/top/about/>ABOUT</a></li></ul></div></div></div></nav><script>var $body=document.body,$toggle=document.querySelector(".navbar-toggle"),$navbar=document.querySelector("#huxblog_navbar"),$collapse=document.querySelector(".navbar-collapse");$toggle.addEventListener("click",handleMagic);function handleMagic(){$navbar.className.indexOf("in")>0?($navbar.className=" ",setTimeout(function(){$navbar.className.indexOf("in")<0&&($collapse.style.height="0px")},400)):($collapse.style.height="auto",$navbar.className+=" in")}</script><style type=text/css>header.intro-header{background-image:url(/img/forest.jpg)}</style><header class=intro-header><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=post-heading><div class=tags><a class=tag href=/tags/programming title=Programming>Programming</a>
<a class=tag href=/tags/testing title=Testing>Testing</a></div><h1>Unit Tests that Don't Suck</h1><h2 class=subheading></h2><span class=meta>Posted by
Graham Wheeler's Random Forest
on
Saturday, June 26, 2021</span></div></div></div></div></header><article><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
post-container"><h2 id=introduction>Introduction</h2><p>This post is based on a talk I gave to my team in an effort to establish a common approach to thinking about unit tests. The existing code base we had suffered from a number of problems relating to how tests were being written; despite good intentions, it can be easy to do testing badly. In particular, here are some of the things I observed:</p><ul><li>a massive overuse of dependency injection: pretty much all dependencies of all classes were being set up using DI. I believe this was related to the next point about the overuse of mocks. DI is certainly useful, especially when wiring up a high-level architecture, but it does not need to be used for everything;</li><li>a massive overuse of mocking. This was the main problem I observed. I went so far as to write a simple Python script to generate some metrics around this, and in the most egregious cases I found tests that had over 60 mocked classes being created. At that level it is very difficult to even know what is being tested; are you just testing that your mocks exhibit the behavior you specified in the mocks? What real behavior is being tested?</li><li>an overuse of classes. This was TypeScript code; top-level functions are allowed, and can be encapsulated in namespaces; you don&rsquo;t have to put everything in a class;</li><li>treating unit tests as testing units of code rather than units of behavior. This meant a lot more internal implementation details were being exposed than necessary, and there was too much &ldquo;unit testing&rdquo; and not enough functional and integration testing;</li><li>there was disagreement within the team about which of the above were problematic and a lack of consistency in approach that was unproductive.</li></ul><p>As a result, writing tests was hard, code reviews could be acrimonious, the design was overcomplicated and opaque, and the tests were not actually even testing that much. This was despite the fact that a lot of effort was going in to writing tests. It was clear that we needed to do better, and that meant starting out with a common approach and more education for junior developers who would otherwise just emulate existing patterns and practices. To address this, I started by giving a talk to the entire team on better testing practices. The below is my opinion, but I can attest to the fact that after these practices were adopted in a large new piece of code, we got major improvements in ease of test writing, effectiveness of tests, comprehensibility of tests, and overall architecture.</p><h2 id=why-test>Why Test?</h2><p>If we want to sustain velocity, including in the face of changing requirements, we need to:</p><ul><li>avoid regressions and wasting time on finding bugs in existing code - so we need to be able to detect regressions quickly and pinpoints their origin accurately</li><li>not accrue large amounts of technical debt, but instead keep our code simple to understand and modify. We need confidence that continuous refactoring is not going to break functionality, and tests can provide that confidence.</li></ul><p>In addition:</p><ul><li>tests are <em>executable documentation</em> - if you want to know what the code does in a certain case, the tests can tell you</li><li>a passing test suite can reduce the effort required in code review, as reviewers don&rsquo;t have to manually verify that code behaves as intended (corollary: as a reviewer, you should demand quality tests)</li><li>tests exercise our APIs and help to ensure that they are well-designed and usable</li></ul><p>So tests are clearly good, but&mldr;</p><h2 id=tales-from-the-trenches>Tales from the Trenches</h2><p><em>It&rsquo;s morning: you change a few lines of code and a bunch of tests break. None of the tests are catching an actual bug in the code you changed; they just made assumptions about implementation (implicit contracts) that were broken. But you have to try to understand, check and fix each one anyway, and the band-aids you apply make the tests even less readable and less useful. It&rsquo;s now the end of the day and you still haven&rsquo;t added new tests or validated your actual change. You were mostly just &lsquo;fixing&rsquo; broken tests.</em></p><p><em>You get an alert that the CI build failed. You go take a look. Several tests failed. You recognize the first two as flaky tests once again flaking out. The next one is an old test that has broken for the first time in a long time. &lsquo;Great!&rsquo;, you think, &lsquo;We caught a bug!&rsquo; But you look at the test and can&rsquo;t for the life of you figure out what it&rsquo;s actually doing. The last one takes a while to understand, but once you do you realize it wasn&rsquo;t even testing any code, it was just &rsquo;testing&rsquo; the mocks. You disable all four tests and move on.</em></p><p>What value were these tests?</p><p>The tests in these scenarios had the opposite effect of what good tests should have:</p><ul><li>they were unclear in intent, and drained productivity and morale as they were hard to read and understand</li><li>they did not help to improve the design of the system</li><li>they failed to find any actual bugs</li><li>they were brittle - they failed as a result of an unrelated change that didn&rsquo;t introduce new bugs</li><li>they were unreliable</li></ul><p>Such tests are all cost with no benefit. Much of this post is about recognize good vs bad tests to avoid these kinds of situations. But we&rsquo;ll start with an overview of testing in general with a particular focus on unit tests.</p><h2 id=unit-tests>Unit Tests</h2><p>A unit test is an automated test (of a <em>system under test</em> or SUT) that:</p><ul><li>verifies a single unit (of behavior or of code; see schools)</li><li>executes fast</li><li>and in isolation from other tests (so you can run/rerun tests individually in any order)</li></ul><p>Isolation is achieved with the help of <em>test doubles</em>.</p><p>Unit tests have a 3-part structure:</p><ul><li><em>Arrange</em>: bring the system under test (SUT) and its dependencies to a desired state</li><li><em>Act</em>: call methods on the SUT, pass the prepared dependencies, and capture the output value (if any).</li><li><em>Assert</em>: verify the outcome (the return value, the final state of the SUT and its collaborators, or the methods the SUT called on those collaborators).</li></ul><p>I prefer the alternative nomenclature: <em>Given/When/Then</em>, as this form encourages a more declarative writing style and thus tests that are clearer to read.</p><p>If you&rsquo;re doing test-driven development (TDD) you may write the <em>Then</em> section first and then figure out the first two sections.</p><p>Unit tests can be good or bad. We&rsquo;ve seen examples above already of bad tests. The differences are pretty clear:</p><table><thead><tr><th>Good Tests</th><th>Bad Tests</th></tr></thead><tbody><tr><td>Automatic</td><td>Manual (not always bad)</td></tr><tr><td>Reliable</td><td>Flaky</td></tr><tr><td>Fast to execute</td><td>Slow to execute</td></tr><tr><td>Clear (useful as documentation)</td><td>Hard to understand</td></tr><tr><td>Tests high-value code</td><td>Tests the wrong things</td></tr><tr><td>Sensitive to regressions</td><td>Poor at catching regressions</td></tr><tr><td>Resilient to changes in implementation</td><td>Brittle</td></tr><tr><td>Clearly identifies the failure</td><td>Hard to interpret</td></tr><tr><td>Easy to write</td><td>Difficult to write</td></tr><tr><td>Easy to set up</td><td>Hard to set up</td></tr></tbody></table><h2 id=test-doubles>Test Doubles</h2><p>We often want to isolate behavior when testing, and/or may need to replace slow, stateful systems like databases; we do this with <em>test doubles</em>, which are fake dependencies such as:</p><ul><li><em>stubs</em> that emulate calls the SUT makes to dependencies to get input data. Types of stubs include <em>dummies</em> which just return hard-coded values, and <em>fakes</em> which replace dependencies that don&rsquo;t yet exist (common in TDD), or high-fidelity simulations of systems that are broadly used (e.g. file system or database fakes);</li><li><em>mocks</em> which emulate calls the SUT makes to dependencies that change the state of those dependencies.</li></ul><p>Mocking libraries are used to create both mocks and stubs, so when we &ldquo;mock&rdquo; something we may be doing either of these; it&rsquo;s an unfortunate overload of the term.</p><p>Bertrand Meyer, the creator of the Eiffel language, came up with the <em>Command-Query Separation Principle</em>, that can help with having a cleaner design and has an impact on testing: <em>Every method should be either a command (with side effects on state but returning no value) or a query (with no side effects, returning a value), but not both.</em></p><p>In other words, clearly separate methods that change state from those that don&rsquo;t. You can then use queries in many situations and in different orders with little risk, and focus much more of your attention on getting commands correct. Of course, every rule has exceptions: e.g. stack.Pop() which changes state and returns a value.</p><p>In the context of Test Doubles: a method in a test double can be a stub or a mock but shouldn&rsquo;t be both.</p><h2 id=types-of-test-validation>Types of Test Validation</h2><p>There are three main types of unit test checks used in the Assert/Then part of the test:</p><ul><li>output-based or <em>functional testing</em> where you feed an input to the SUT and check the output it produces;</li><li><em>state-based testing</em> that verifies the public state of the system after an operation is completed (&ldquo;what&rdquo; checks);</li><li><em>interaction-based testing</em>, where you use mocks to verify communications between the system under test and its collaborators (&ldquo;how&rdquo; checks).</li></ul><p>Functional testing is the easiest and cleanest. Interaction-based tests are the most brittle, as we often change how our code works, and interaction tests are typically tied to implementation. Interaction tests often rely on mocks, so when the code changes the mock often must be changed too. It&rsquo;s good practice to think about how you can avoid interaction tests and instead convert them to functional or state-based tests.</p><h2 id=test-driven-development-tdd>Test-Driven Development (TDD)</h2><p>You can&rsquo;t really talk about testing without talking about TDD.</p><p>The typical workflow for TDD is:</p><ul><li>Outer loop: write one or more failing acceptance tests for the feature that is about to be developed. These tests may take multiple checkins before they pass, and are excluded from CI until they are passing. Use stubs for dependencies that don’t yet exist; the stub will evolve as the desired API for that dependency emerges;</li><li>Inner loop: write one or more failing unit tests or integration tests for the class/method or other unit that is about to be written, then write the code to make that pass. All unit tests must be passing before checking in, and are run as part of CI.</li></ul><p>Reliable acceptance tests are often hard to write after the fact. Doing them upfront helps make sure we create testable software.</p><p>Acceptance tests ideally exercise the system end-to-end without directly calling internal code or making assumptions about implementation - so they go through user interface, public API, web service, etc. Even better is if these tests include the process by which the system is built and deployed. So if possible, you want a check in/merge to master to:</p><ul><li>check out latest version, compile and unit test the code</li><li>integrate and package the system, and perform a production-like deployment into a realistic environment</li><li>exercise the system through its external access points to run the acceptance tests for completed functionality</li></ul><p>TDD is a useful practice because:</p><ul><li>writing tests first is a <em>design activity</em> that clarifies acceptance criteria, encourages loose coupling, adds an executable specification of the code&rsquo;s purpose, all while adding to the regression suite and letting us know when we have done enough (YAGNI);</li><li>developers are often bad at writing sufficient quality tests after the code is written and apparently working, and writing them afterwards accrues only a few of the above benefits.</li></ul><p>In addition, for the very first acceptance test, we must have implemented a whole automated build, deploy, and test cycle. This is a lot of work to do before we can even see our first test fail, but deploying and testing right from the start of a project forces the team to understand how their system fits into the world. It flushes out the “unknown unknown” technical and organizational risks so they can be addressed while there’s still time (see also )</p><h2 id=the-testing-schools---classic-vs-london>The Testing Schools - Classic vs &ldquo;London&rdquo;</h2><p>When mocking libraries first became popular, a school of though arose in which it was considered good practice to mock all dependencies. The idea behind this is that this provides the best isolation of the SUT; any test failures are clearly going to be because something is wrong in the SUT and not because of a change in a dependency. This approach came out of a community of testers in London, and became known as the &ldquo;London School&rdquo; or &ldquo;Mockist school&rdquo;.</p><p>There is certainly value in this approach, but like a lot of good ideas, it can be taken to extremes and become counter-productive. It works best in conjunction with well-designed and decoupled, modular code; if the code is not clean, then following this testing approach can end up being obscurist. Unfortunately, this has become one of those tech &ldquo;holy wars&rdquo; where each side will tend to assume you use the other school&rsquo;s approach badly in order to argue why its wrong. I do believe though that it is easier to fall into the “use badly” bucket when using Mockist approach:</p><ul><li>extensive use of doubles that get out of sync with real implementation can have a significant maintenance cost;</li><li>over-reliance on mocking can mean more integration tests are needed to trust your code;</li><li>code must be designed for test doubles (support for injection etc). This is not free (although in some cases can improve design).</li></ul><p>The London School came out of the TDD world and reflects that; if you are doing TDD you need to use mocks at least in the cases where the real objects don’t yet exist. I think it causes way more problems when testing is done after the fact.</p><p>Some of the differences are:</p><table><thead><tr><th>Classic School</th><th>London School</th></tr></thead><tbody><tr><td>Mock shared mutable dependencies</td><td>Mock all mutable dependencies</td></tr><tr><td>Use CI w/small code deltas and good error messages to find faults</td><td>Test small amounts of isolated code to pinpoint faults</td></tr><tr><td>Prefer state-based tests</td><td>Okay with interaction-based tests</td></tr><tr><td>More &lsquo;black-box&rsquo;</td><td>Prone to coupling tests to implementation details</td></tr><tr><td>Tests are &lsquo;higher-level&rsquo; and better able to detect design problems</td><td>Rely more on acceptance tests for design feedback</td></tr></tbody></table><p>(Shared dependencies here are shared between tests, so the kind of dependencies that may prevent tests from being run in parallel or out-of-order; a typical example would be a database or file system).</p><p>My advice is: if you&rsquo;re doing TDD/BDD you&rsquo;ll likely need to adopt a more Mockist approach but if not, the Classical School can let you test more surface with less scaffolding and make it easier to avoid brittle tests (e.g. from an over-reliance on mocking).</p><p>It&rsquo;s worth noting this quote from Andrew Trenk and Dillon Bly of Google:</p><p><em>“One lesson we learned the hard way is the danger of over-using mocking frameworks. [At first] they seemed like a hammer fit for every nail - they made it very easy to write highly focused tests against isolated pieces of code without having to worry about how to construct the dependencies of the code. …Several years and countless tests later … we began to realize the costs of such tests. Though these tests were easy to write, we suffered greatly given that they required constant effort to maintain while rarely finding bugs. [Now] many engineers avoid mocking frameworks in favor of writing more realistic tests.”</em></p><p>So, when should you use mocks? Aside from mutable shared dependencies, other possible good use cases are:</p><ul><li>replacing slow components</li><li>replacing non-deterministic components</li><li>replacing resource-intensive components or hard to initialize/configure components</li><li>generating stub values that would be hard to create with the real code</li><li>in TDD, stubbing dependencies that don&rsquo;t yet exist (but consider replacing these later with real code)</li></ul><p>Otherwise IMO you should prefer using real code as much as possible.</p><p>The rest of this post will dive into the other attributes of good tests with some advice on how to achieve these.</p><h2 id=unit-tests-should-test-units-of-behavior>Unit Tests should test Units of Behavior</h2><p>The main mistake I have seen made with unit tests has been thinking that unit tests should be testing &ldquo;units of code&rdquo; (whether that be functions, methods or classes). That is a slippery slope that is going to lead to a very brittle architecture and a lot of confusion and debate about what should and shouldn&rsquo;t be tested. If you wrote tests for every function you would be writing an order of magnitude more test code than product code, and the maintenance burden would be immense.</p><p>Instead, you want to test &ldquo;units of behavior&rdquo;, and in particular, units of behavior as expressed in things like user stories or their first level breakdown into steps. What is a meaningful action that can be taken using the public API of a system? That is a good abstraction for thinking about a test. You want to avoid thinking about implementation (the &ldquo;how&rdquo;), and thinking instead about functionality (the &ldquo;what&rdquo;).</p><h2 id=tests-should-be-reliable>Tests should be Reliable</h2><p>When we say that tests should be reliable, we mean that they should not be flaky, and should not produce false positives (test fails but the code is correct - often this may be because the test is using interaction testing that is tied to the implementation and the implementation changed) or false negatives (code is wrong but test passes anyway; this may be because the code was changed and introduced bugs but the test was poorly constructed and is not checking the right things).</p><p>To improve reliability, strive for functional or state-based checks, check boundary conditions, reduce the use of mocks, especially of volatile components, <em>but</em> do use mocks in the case of nondeterministic or unreliable dependencies to avoid tests being flaky.</p><h2 id=tests-should-be-fast-and-automated>Tests should be Fast and Automated</h2><p>If tests run automatically and run fast they will be run often, and the converse is also true, so we want to make sure the cost of running tests is low. You can profile you code or use domain knowledge to identify slow components that should be replaced by test doubles. If you are testing async code and have to do that by polling for a state change, make sure to use short polling intervals or - better - find a way to use a callback instead to signal completion.</p><p>If the tests are not fast, and can’t be made fast, be more selective about when they are executed. For example, Google classifies tests as unit, integration or end-to-end, but separately also classifies them as small, medium or large depending on resources required, including time:</p><table><thead><tr><th>Size</th><th>Constraints</th><th>When Run</th><th>Ratio</th></tr></thead><tbody><tr><td>Small</td><td>Single thread, no blocking I/O (can use fakes)</td><td>Before commits</td><td>80%</td></tr><tr><td>Medium</td><td>Single host (127.0.0.1 only)</td><td>Before merges</td><td>15%</td></tr><tr><td>Large</td><td>Unconstrained</td><td>As part of CI releases</td><td>5%</td></tr></tbody></table><h2 id=tests-should-prioritize-high-value-code>Tests should prioritize High-Value Code</h2><p>We can classify code according to complexity (e.g. cyclomatic) and number of dependencies:</p><table><thead><tr><th>Complexity</th><th>Few Dependencies</th><th>Many Dependencies</th></tr></thead><tbody><tr><td>Low Complexity</td><td>Trivial code</td><td>Controllers/orchestrators</td></tr><tr><td>High Complexity</td><td>Domain model, algorithms, etc</td><td>Big balls of mud</td></tr></tbody></table><ul><li><em>Trivial code</em> may not need or be worth testing. It’s certainly low priority.</li><li><em>Controllers/orchestrators</em> are best tested with integration and end-to-end tests, rather than creating complex test setups with lots of mocks.</li><li><em>Domain model code and complex algorithms</em> are where we get the most bang for the buck with unit tests; focus on these first.</li><li><em><a href=https://en.wikipedia.org/wiki/Big_ball_of_mud>Big balls of mud</a></em> should be refactored/separated into controller code and domain code if possible, as a higher priority task than test coverage.</li></ul><h2 id=tests-should-be-sensitive-to-regressions>Tests should be Sensitive to Regressions</h2><p>The best way to do this is to test as much real code as possible, and make sure to test for boundary conditions/edge cases. This is where overuse of mocks will hurt you, especially if you change the code being mocked and the mocks are not changed to reflect that.</p><p>Edge cases are inputs that the specification for your function allows but that might be tricky to handle or unexpected in practice. They’re both a common source of bugs in and of themselves, but are also useful in testing because they can span the expected range of behavior of a function. Some common edge cases with ints are 0, -1, 1, minint, and maxint, while for arrays, you should try null, the empty array, an array of length 1, and a very long array, for example.</p><p>A good practice when fixing regression bugs that were not caught by tests is to add new tests to make sure a similar regression will be caught in the future.</p><h2 id=tests-should-be-resistant-to-change>Tests should be Resistant to Change</h2><p><em>Brittle tests</em> - tests that break due to unrelated code changes - have a high maintenance cost and low value. We should be able to change underlying implementations - e.g. due to automated refactoring - without tests starting to fail. If they do fail, that means we have changed the behavior, or we have written the tests at the wrong level of abstraction, or the tests are too reliant on implementation details. In the latter two cases we should improve the quality of the tests.</p><p>Only when <em>changing existing behavior</em> should we expect that some tests might break and need to be updated. Bug fixes should not clearly break existing tests unless the tests themselves were wrong. Nor should adding new features (unless we are doing TDD and these are red/green tests).</p><p>If your tests do break when you change or add code that should not affect existing behavior, as yourself <a href="https://www.youtube.com/watch?v=h4XMoHhireY">what can you learn?</a>.</p><p>Interaction tests are frequently brittle as they rely on implementation details, so they are best avoided. As much as possible, tests should behave like regular clients of the code and <a href=https://testing.googleblog.com/2015/01/testing-on-toilet-prefer-testing-public.html>only call public APIs</a>, which are explicit contracts and are generally much more stable than implementations; if a test breaks when calling the public API that guarantees users of the code would be similarly affected.</p><p>What constitutes a &lsquo;public API&rsquo; is context-dependent. Helper classes may have public methods but if used only in limited places may not count. APIs of libraries that are to be used by 3rd parties definitely count. There is a gray area in between that often depends on org structure.</p><p>Related to the above, you want to test at architectural &ldquo;seams&rdquo; (public APIs being examples). Your system is made up of components that interact with each other, and those interaction points are the natural boundaries at which you should be thinking about testing. Avoid the temptation to create artificial seams just for testing purposes, as this can cause technical debt and architectural brittleness later, as well as simply muddying the real structure of the system. If you are making private members public just to support testing you are probably doing something wrong.</p><h2 id=tests-should-clearly-identify-issues>Tests should Clearly Identify Issues</h2><p>When a test fails, you now have the challenge of figuring out what the failure means. The more information you can get at this point the better. You would also like to pinpoint the part of the code responsible, and this is why the London school like to use mocks extensively, the theory being that the amount of code under test is smaller and so it is easier to find an issue (however, there are significant trade-offs as I hope the earlier discussion has made clear).</p><p>The biggest help here is going to be using good assertion libraries that provide clear failure messages. Examples in the Java world are Hamcrest, AssertJ, or Google Truth. Avoid simplistic <code>assertTrue</code>/<code>assertFalse</code> asserts which are low-information. For example:</p><pre tabindex=0><code>// Google Truth (https://github.com/google/truth)

assertThat(names).contains(&#34;Graham&#34;);
assertThat(first.endDate()).isNotEqualTo(second.endDate()));
</code></pre><p>is much better than:</p><pre tabindex=0><code>// jUnit
assertTrue(names.contains(&#34;Graham&#34;)); // &#34;expected TRUE&#34; is not very useful
assertFalse(first.endDate().equals(second.endDate()));
</code></pre><p>For example, below is an error message generated by Google Truth; you can see it provides extensive context around the failure:</p><pre tabindex=0><code>assertThat(projectsByTeam())
 .valuesForKey(&#34;corelibs&#34;)
 .containsExactly(&#34;guava&#34;, &#34;dagger&#34;, &#34;truth&#34;, &#34;auto&#34;, &#34;caliper&#34;);

value of : projectsByTeam().valuesForKey(corelibs)
missing (1) : truth

───

expected : \[guava, dagger, truth, auto, caliper\]
but was : \[guava, auto, dagger, caliper\]
multimap was: {corelibs\=\[guava, auto, dagger, caliper\]}
 at com.google.common.truth.example.DemoTest.testTruth(DemoTest.java:71)
</code></pre><p>You should also consider overriding classes used in test data with subclasses that then override the <code>toString()</code> (or equivalent) method to give a more meaningful &rsquo;expected&mldr;but got&rsquo; assertion message. For example, if we create <code>Date</code> objects to specify a range, we could use an override <code>NamedDate</code> class so we can name the dates, like &ldquo;start date&rdquo; and &ldquo;end date&rdquo;. Some mock libraries allow named mocks that achieve a similar effect.</p><p>While I have generally argued against using interaction assertions, they do provide more behavioral information than state or functional assertions, so if you have to use them put them first so that they are more likely the failures that get reported, as they may give you a better clue as to what went wrong.</p><p>Another good way to help narrow down the code that caused a test failure is to make sure you do do frequent small pushes to source control so when a test fails unexpectedly you can at worst know that there is not a lot of code to inspect to find the problem and at best just quickly revert, which may be more efficient than spending time trying to debug the problem.</p><p>Clarity and efficiency is lost not only when the reason for a failure is not obvious, but also when the reason for the test is not obvious, so name tests clearly in a way that describes the point of each test case and its differences from the other test cases (e.g. use <a href=https://en.wikipedia.org/wiki/TestDox>testdox</a>). As tests are typically discovered and called via reflection there is little cost to having long meaningful names that describe what is being tested and the expected behavior.</p><h2 id=tests-should-be-easy-to-read>Tests should be Easy to Read</h2><p>For tests to be easy to read, they should be as self-contained as possible. Aim for clarity and brevity. If you use arrange or assert test helpers (which can help with brevity), name them very clearly so that their purpose is clear just from the name; the test body should be readable in isolation without having to understand a bunch of helper methods unless those are very clear from their names. Avoid handling exceptions unless you are specifically testing that an exception is being thrown. Try to keep the code path linear, with no loops or branches; you may be able to achieve the same effect with parameterized tests or through using multiple tests, and a failure will be more clear while still improving readability.</p><p>Avoid hidden globals or singletons; these are semi-hidden dependencies that may be non-obvious to the reader. You can inject these instead to make them local and explicit. Similarly, test suite setup code is hidden context and best avoided; if you use it, use it to construct stateless objects. Be particularly careful about not having your test suite setup code be the code that creates the state that you are later testing for.</p><p>If you make use of literal constants, assign them to variables with names that express their intent before using them.</p><p>As tests have no tests themselves, clarity is key! It&rsquo;s okay to duplicate code in service of readability. <a href=https://testing.googleblog.com/2019/12/testing-on-toilet-tests-too-dry-make.html>https://testing.googleblog.com/2019/12/testing-on-toilet-tests-too-dry-make.html</a></p><p>Focus on the most important assertions; if you assert on all the resulting state you may obscure the true purpose of the test, or fail due to an unrelated issue. Assert helpers, if used, should be focused on single relevant facts and not bundle up multiple different checks.</p><h2 id=tests-should-be-easy-to-write>Tests should be Easy to Write</h2><p>If a feature is difficult to test, ask why. Are you testing at the right level of abstraction? Should the test be written at a higher/lower level? Should the code itself be refactored to be easier to test?</p><p>While generally you want to avoid relying on test helpers as they can obscure the code, an exception is test data helpers. These can be <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.18.4710&rep=rep1&type=pdf">written in a style that is declarative</a> and easy to read so they are not only convenient but help with readability.</p><h2 id=measuring-the-quality-of-our-tests>Measuring the Quality of our Tests</h2><p>We&rsquo;ve spoken about the importance of having tests that are sensitive to regressions; i.e. do the tests actually find bugs? How can we measure this? One way might be:</p><ul><li>How many bugs did we find through tests?</li><li>How many bugs made it to production?</li><li>What’s the ratio of the above two numbers?</li></ul><p>This is a bit too squishy, especially the first number, unless we are diligent about tracking, and it is not easy at all to automate. In addition, its a lagging metric so we can only tell how good the tests <em>have been</em>, not how good they <em>are</em>. We need something better.</p><p><em>Mutation testing</em> involves modifying our code small ways, using mutation operators that mimic programming errors. Each mutated version is called a mutant. If a previously passing test fails when run against a mutant, we say it <em>kills</em> the mutant; if no test kills the mutant, the mutant <em>survives</em>. The <em>mutation index</em> is the percentage of mutants killed by the test suite.</p><p>Apart from tracking the mutation index as a general metric for test quality, we get additional value from mutation testing. Tests that kill no mutants are weak tests and could be improved or removed. Code with surviving mutants is weakly tested code and new tests can be added to kill more of the mutants.</p><p>That said, there are limitations. Some mutations still produce same result as unmutated code and will never be caught. The bugs introduced by mutation are small and localized (often inverting conditionals) and may not be reflective of real-world human error.</p><p>Mutation testing is computationally expensive, but good tools only mutate the code covered by a test and it is now quite practical; still, you should expect an order of magnitude or more increase in the time for a test run. There are a number of tools available (see <a href=https://en.wikipedia.org/wiki/Mutation_testing>https://en.wikipedia.org/wiki/Mutation_testing</a>). The most mature of these is <a href=https://stryker-mutator.io/>Stryker</a>.</p><h2 id=code-reviews>Code Reviews</h2><p>Questions to ask when reviewing code:</p><ul><li>are there tests? If not, why not? Except in exceptional cases there should always be tests;</li><li>read the tests first and again at the end. If properly done, they should help you understand the code you are going to review. They should also give you confidence that the code you are reviewing works so you don&rsquo;t have to verify that yourself</li><li>can I understand the tests? Are they clear in intent? Are they testing the right things?</li><li>do the tests match the requirements for the user story?</li><li>is the changed code covered by tests? Are there areas missing coverage? Should that be addressed?</li><li>do the tests meet the other attributes described in this post? Can they be improved?</li></ul><h2 id=further-reading>Further Reading</h2><ul><li>Flaky tests at Google and how we mitigate them - Google - <a href=https://testing.googleblog.com/2016/05/flaky-tests-at-google-and-how-we.html>https://testing.googleblog.com/2016/05/flaky-tests-at-google-and-how-we.html</a> (lots more good stuff at <a href=https://testing.googleblog.com/>https://testing.googleblog.com/</a>)</li><li>Mocks aren&rsquo;t Stubs - Martin Fowler - <a href=https://martinfowler.com/articles/mocksArentStubs.html>https://martinfowler.com/articles/mocksArentStubs.html</a> - good discussion of the two schools and other topics</li><li>Mocking is a Code Smell – Eric Elliott - <a href=https://medium.com/javascript-scene/mocking-is-a-code-smell-944a70c90a6a>https://medium.com/javascript-scene/mocking-is-a-code-smell-944a70c90a6a</a> – some JS specific advice</li><li>(Google) Automated Testing Playbook - <a href=https://github.com/mbland/automated-testing-playbook>https://github.com/mbland/automated-testing-playbook</a></li><li><a href=https://amzn.to/3FJT6rm>&ldquo;Unit Testing: Principles, Practices and Patterns&rdquo;</a>, Vladimir Khorikov - an excellent classical school book that covers many of the topics in this post</li><li><a href=https://amzn.to/3eCAXzY>&ldquo;Growing Object-Oriented Software, Guided by Tests&rdquo;</a>, Steve Freeman and Nat Pryce - a good book on London-school style TDD</li></ul><hr><ul class=pager><li class=previous><a href=/post/prioritization-estimating-and-planning/ data-toggle=tooltip data-placement=top title="Prioritization, Estimating and Planning">&larr;
Previous Post</a></li><li class=next><a href=/post/moving-to-hugo/ data-toggle=tooltip data-placement=top title="Moving my blog to Hugo">Next
Post &rarr;</a></li></ul></div><div class="col-lg-2 col-lg-offset-0
visible-lg-block
sidebar-container
catalog-container"><div class=side-catalog><hr class="hidden-sm hidden-xs"><h5><a class=catalog-toggle href=#>SECTIONS</a></h5><ul class=catalog-body></ul></div></div><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
sidebar-container"><section><hr class="hidden-sm hidden-xs"><h5><a href=/tags/>FEATURED TAGS</a></h5><div class=tags><a href=/tags/data-science title=data-science>data-science</a>
<a href=/tags/jupyter title=jupyter>jupyter</a>
<a href=/tags/management title=management>management</a>
<a href=/tags/pandas title=pandas>pandas</a>
<a href=/tags/programming title=programming>programming</a>
<a href=/tags/psychology title=psychology>psychology</a>
<a href=/tags/python title=python>python</a></div></section><section><hr><h5>FRIENDS</h5><ul class=list-inline><li><a target=_blank href=https://snarky.ca/author/brett/>Brett Cannon</a></li><li><a target=_blank href=http://journal.stuffwithstuff.com/>Bob Nystrom</a></li></ul></section></div></div></div></article><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src=https://utteranc.es/client.js repo=gramster/gramw.github.io issue-term=title theme=github-light crossorigin=anonymous async></script><footer><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><ul class="list-inline text-center"><li><a target=_blank href=https://github.com/gramster><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-github fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=https://www.linkedin.com/in/grahamwheeler><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-linkedin fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=https://stackoverflow.com/users/968133><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-stack-overflow fa-stack-1x fa-inverse"></i></span></a></li><li><a href rel=alternate type=application/rss+xml title="Graham Wheeler's Random Forest"><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-rss fa-stack-1x fa-inverse"></i></span></a></li></ul><p class="copyright text-muted">Copyright &copy; Graham Wheeler's Random Forest 2022<br><a href=https://themes.gohugo.io/hugo-theme-cleanwhite>CleanWhite Hugo Theme</a> by <a href=https://zhaohuabing.com>Huabing</a> |
<iframe style=margin-left:2px;margin-bottom:-5px frameborder=0 scrolling=0 width=100px height=20px src="https://ghbtns.com/github-btn.html?user=zhaohuabing&repo=hugo-theme-cleanwhite&type=star&count=true"></iframe></p></div></div></div></footer><script>function loadAsync(i,t){var n=document,s="script",e=n.createElement(s),o=n.getElementsByTagName(s)[0];e.src=i,t&&e.addEventListener("load",function(e){t(null,e)},!1),o.parentNode.insertBefore(e,o)}</script><script>$("#tag_cloud").length!==0&&loadAsync("/js/jquery.tagcloud.js",function(){$.fn.tagcloud.defaults={color:{start:"#bbbbee",end:"#0085a1"}},$("#tag_cloud a").tagcloud()})</script><script>loadAsync("https://cdn.jsdelivr.net/npm/fastclick@1.0.6/lib/fastclick.min.js",function(){var e=document.querySelector("nav");e&&FastClick.attach(e)})</script><script type=text/javascript>function generateCatalog(e){_containerSelector="div.post-container";var t,n,s,o,i,r=$(_containerSelector),a=r.find("h1,h2,h3,h4,h5,h6");return $(e).html(''),a.each(function(){n=$(this).prop("tagName").toLowerCase(),i="#"+$(this).prop("id"),s=$(this).text(),t=$('<a href="'+i+'" rel="nofollow">'+s+"</a>"),o=$('<li class="'+n+'_nav"></li>').append(t),$(e).append(o)}),!0}generateCatalog(".catalog-body"),$(".catalog-toggle").click(function(e){e.preventDefault(),$(".side-catalog").toggleClass("fold")}),loadAsync("/js/jquery.nav.js",function(){$(".catalog-body").onePageNav({currentClass:"active",changeHash:!1,easing:"swing",filter:"",scrollSpeed:700,scrollOffset:0,scrollThreshold:.2,begin:null,end:null,scrollChange:null,padding:80})})</script></body></html>